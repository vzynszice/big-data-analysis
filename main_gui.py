import sys
import shlex
from PyQt5.QtWidgets import (QApplication, QWidget, QVBoxLayout, QHBoxLayout, 
                             QPushButton, QLabel, QComboBox, QTextEdit, QListWidget,
                             QFileDialog, QMessageBox, QLineEdit, QInputDialog)
from PyQt5.QtCore import Qt, QThread, pyqtSignal
import subprocess
import os
import stat 
import time

try:
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from config import config
    EMR_MASTER_DNS = config.EMR_MASTER_DNS
    EMR_KEY_PATH = config.EMR_KEY_PATH
    EMR_SSH_USER = config.EMR_SSH_USER
    S3_CODE_BUCKET = config.S3_CODE_BUCKET
except ImportError:
    print("UYARI: config.py bulunamadÄ±. VarsayÄ±lan deÄŸerler kullanÄ±lÄ±yor.")

def execute_remote_ssh_command(command_str, window_for_logging=None):
    """
    Verilen komutu EMR master node'unda SSH Ã¼zerinden Ã§alÄ±ÅŸtÄ±rÄ±r.
    stdout ve stderr'i yakalar.
    """
    if not EMR_MASTER_DNS or not EMR_KEY_PATH:
        if window_for_logging:
            log_message(window_for_logging, "HATA: EMR Master DNS veya Key Path ayarlanmamÄ±ÅŸ.")
        return None, "EMR baÄŸlantÄ± bilgileri eksik."

    if not os.path.exists(EMR_KEY_PATH):
        if window_for_logging:
            log_message(window_for_logging, f"HATA: SSH anahtar dosyasÄ± bulunamadÄ±: {EMR_KEY_PATH}")
        return None, f"SSH anahtar dosyasÄ± bulunamadÄ±: {EMR_KEY_PATH}"
    
    try:
        with open(EMR_KEY_PATH, 'r') as f:
            first_line = f.readline()
            if window_for_logging:
                log_message(window_for_logging, f"SSH anahtar dosyasÄ± okunabilir durumda.")
    except Exception as e:
        if window_for_logging:
            log_message(window_for_logging, f"HATA: SSH anahtar dosyasÄ± okunamÄ±yor: {e}")
        return None, f"SSH anahtar dosyasÄ± okunamÄ±yor: {e}"
    try:
        file_stat = os.stat(EMR_KEY_PATH)
        file_mode = stat.S_IMODE(file_stat.st_mode)
        
        if file_mode != 0o400:
            if window_for_logging:
                log_message(window_for_logging, f"SSH anahtar dosyasÄ± izinleri: {oct(file_mode)}, dÃ¼zeltiliyor...")
            os.chmod(EMR_KEY_PATH, 0o400)
            if window_for_logging:
                log_message(window_for_logging, "SSH anahtar dosyasÄ± izinleri 400 olarak ayarlandÄ±.")
    except Exception as e:
        if window_for_logging:
            log_message(window_for_logging, f"UYARI: Dosya izinleri kontrol edilemedi: {e}")

    ssh_command = [
        "ssh",
        "-o", "StrictHostKeyChecking=no",
        "-o", "UserKnownHostsFile=/dev/null",
        "-o", "PasswordAuthentication=no",
        "-o", "IdentitiesOnly=yes",
        "-i", EMR_KEY_PATH,
        f"{EMR_SSH_USER}@{EMR_MASTER_DNS}",
        command_str
    ]
    
    if window_for_logging:
        log_message(window_for_logging, f"SSH komutu hazÄ±rlandÄ±.")
        log_message(window_for_logging, f"Komut: {' '.join(shlex.quote(c) for c in ssh_command[:6])}...")

    try:
        process = subprocess.Popen(
            ssh_command, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE, 
            text=True,
            env=os.environ.copy()
        )
        stdout, stderr = process.communicate(timeout=600)
        
        if window_for_logging:
            if stdout:
                log_message(window_for_logging, "--- Uzak Komut STDOUT ---")
                log_message(window_for_logging, stdout)
            if stderr:
                log_message(window_for_logging, "--- Uzak Komut STDERR ---")
                log_message(window_for_logging, stderr)
        
        if process.returncode != 0:
            if window_for_logging:
                log_message(window_for_logging, f"HATA: Uzak komut {process.returncode} ile sonlandÄ±.")
            return None, stderr
            
        return stdout, stderr
        
    except subprocess.TimeoutExpired:
        if window_for_logging:
            log_message(window_for_logging, "HATA: Uzak komut zaman aÅŸÄ±mÄ±na uÄŸradÄ±.")
        process.kill()
        stdout, stderr = process.communicate()
        return None, "Zaman aÅŸÄ±mÄ±" + stderr
    except Exception as e:
        if window_for_logging:
            log_message(window_for_logging, f"HATA: Subprocess Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rken istisna: {type(e).__name__}: {e}")
        return None, str(e)


app = None

def init_ui(window):
    """
    GeliÅŸmiÅŸ big data analiz aracÄ± iÃ§in kullanÄ±cÄ± arayÃ¼zÃ¼nÃ¼ baÅŸlatÄ±r.
    Bu fonksiyon enterprise-grade data platform'larÄ±n kullandÄ±ÄŸÄ± 
    kategorize edilmiÅŸ veri seÃ§im sistemini implement eder.
    """
    window.setWindowTitle('BLM4120/4821 - Big Data Analiz AracÄ±')
    window.setGeometry(100, 100, 900, 700)  

    main_layout = QVBoxLayout()
    window.setLayout(main_layout)

    # =================================================================
    # 1. GELÄ°ÅMÄ°Å VERÄ° SETÄ° SEÃ‡Ä°M SÄ°STEMÄ°
    # =================================================================
    
    data_selection_group = QVBoxLayout()
    
    # Kategori seÃ§imi - kullanÄ±cÄ±nÄ±n hangi tÃ¼r analiz yapacaÄŸÄ±nÄ± belirler
    category_layout = QHBoxLayout()
    lbl_category = QLabel('Veri Kategorisi:')
    lbl_category.setMinimumWidth(120)  # Consistent label width iÃ§in
    combo_categories = QComboBox()
    
    # Veri kategorileri - gerÃ§ek big data platform'larÄ±nÄ±n yaklaÅŸÄ±mÄ±nÄ± taklit eder
    categories = [
        "Performance Testing",      # Scalability analysis iÃ§in kÃ¼Ã§Ã¼k sample'lar
        "Full Production Data",     # GerÃ§ek analiz iÃ§in complete dataset'ler  
        "Geographic Specific",      # BÃ¶lgesel analiz iÃ§in filtered veriler
        "Manual Path Entry"         # Advanced user'lar iÃ§in custom path'ler
    ]
    combo_categories.addItems(categories)
    category_layout.addWidget(lbl_category)
    category_layout.addWidget(combo_categories)

    # Specific dataset seÃ§imi - kategori iÃ§indeki available option'lar
    dataset_layout = QHBoxLayout()
    lbl_dataset = QLabel('Veri Seti SeÃ§:')
    lbl_dataset.setMinimumWidth(120)  # Label alignment iÃ§in
    combo_datasets = QComboBox()
    combo_datasets.setMinimumWidth(300)  # Dropdown geniÅŸliÄŸi iÃ§in
    dataset_layout.addWidget(lbl_dataset)
    dataset_layout.addWidget(combo_datasets)

    # HDFS path display - seÃ§ilen dataset'in tam yolunu gÃ¶sterir
    path_layout = QHBoxLayout()
    lbl_hdfs_path = QLabel('HDFS Yolu:')
    lbl_hdfs_path.setMinimumWidth(120)
    entry_hdfs_path = QLineEdit()
    entry_hdfs_path.setStyleSheet("""
        QLineEdit {
            background-color: #2b2b2b;
            color: #ffffff;
            border: 1px solid #555555;
            padding: 4px;
            border-radius: 3px;
        }
        QLineEdit:disabled {
            background-color: #3a3a3a;
            color: #cccccc;
        }
    """)
    path_layout.addWidget(lbl_hdfs_path)
    path_layout.addWidget(entry_hdfs_path)

    # Data selection layout'unu ana layout'a ekle
    data_selection_group.addLayout(category_layout)
    data_selection_group.addLayout(dataset_layout)
    data_selection_group.addLayout(path_layout)
    main_layout.addLayout(data_selection_group)

    # =================================================================
    # 2. Ä°STATÄ°STÄ°KSEL FONKSÄ°YON SEÃ‡Ä°MÄ°
    # =================================================================
    
    function_layout = QHBoxLayout()
    lbl_function = QLabel('Ä°statistiksel Fonksiyon SeÃ§in:')
    lbl_function.setMinimumWidth(120)
    combo_functions = QComboBox()
    
    # Available MapReduce algorithms
    functions = [
        "Min-Max Normalization", 
        "Skewness", 
        "Median", 
        "Standard Deviation", 
        "90th Percentile"
    ]
    combo_functions.addItems(functions)
    function_layout.addWidget(lbl_function)
    function_layout.addWidget(combo_functions)
    main_layout.addLayout(function_layout)

    # =================================================================
    # 3. KONTROL BUTONLARI
    # =================================================================
    
    # Analiz baÅŸlatma butonu - main action trigger
    btn_run = QPushButton('Analizi BaÅŸlat')
    btn_run.setStyleSheet("""
        QPushButton {
            background-color: #4CAF50;
            color: white;
            font-weight: bold;
            padding: 8px;
            border-radius: 4px;
        }
        QPushButton:hover {
            background-color: #45a049;
        }
        QPushButton:disabled {
            background-color: #cccccc;
        }
    """)
    main_layout.addWidget(btn_run)

    # =================================================================
    # 4. DURUM VE LOG ALANI
    # =================================================================
    
    # Log bÃ¶lÃ¼mÃ¼ baÅŸlÄ±ÄŸÄ±
    lbl_status = QLabel('Durum ve Loglar:')
    lbl_status.setStyleSheet("font-weight: bold; margin-top: 10px;")
    
    # Scrollable log text area - execution progress iÃ§in
    text_status_log = QTextEdit()
    text_status_log.setReadOnly(True)
    text_status_log.setMaximumHeight(200)  # Screen space'i optimize etmek iÃ§in
    text_status_log.setStyleSheet("""
        QTextEdit {
            background-color: #2b2b2b;
            color: #ffffff;
            font-family: 'Consolas', monospace;
            font-size: 10px;
        }
    """)
    
    main_layout.addWidget(lbl_status)
    main_layout.addWidget(text_status_log)

    # =================================================================
    # 5. SONUÃ‡ GÃ–RÃœNTÃœLEME ALANI  
    # =================================================================
    
    # Results bÃ¶lÃ¼mÃ¼ baÅŸlÄ±ÄŸÄ±
    lbl_results = QLabel('SonuÃ§lar:')
    lbl_results.setStyleSheet("font-weight: bold; margin-top: 10px;")
    
    # Results display area - analysis output iÃ§in
    text_results = QTextEdit()
    text_results.setReadOnly(True)
    text_results.setStyleSheet("""
        QTextEdit {
            background-color: #2b2b2b;
            color: #ffffff;
            border: 1px solid #555555;
            font-family: 'Consolas', monospace;
            padding: 8px;
            border-radius: 3px;
        }
    """)
    
    main_layout.addWidget(lbl_results)
    main_layout.addWidget(text_results)

    # =================================================================
    # 6. WIDGET REFERANSLARI VE EVENT HANDLING
    # =================================================================
    
    # Widget'larÄ± window object'ine baÄŸla - diÄŸer fonksiyonlardan eriÅŸim iÃ§in
    window.combo_categories = combo_categories
    window.combo_datasets = combo_datasets  
    window.entry_hdfs_path = entry_hdfs_path
    window.combo_functions = combo_functions
    window.text_status_log = text_status_log
    window.text_results = text_results
    window.btn_run = btn_run

    # Event handler connections - user interaction'larÄ± handle etmek iÃ§in
    # Bu connection'lar, dropdown deÄŸiÅŸtiÄŸinde otomatik update'leri saÄŸlar
    combo_categories.currentTextChanged.connect(lambda: update_dataset_options(window))
    combo_datasets.currentTextChanged.connect(lambda: update_hdfs_path_from_selection(window))
    
    # Ana analiz butonunun click event'ini baÄŸla
    btn_run.clicked.connect(lambda: handle_run_analysis(window))

    # =================================================================
    # 7. Ä°NÄ°TÄ°AL STATE SETUP
    # =================================================================
    
    # GUI'yi initial state'e ayarla - default olarak Performance Testing kategorisi
    update_dataset_options(window)  # Ä°lk kategori iÃ§in dataset'leri yÃ¼kle
    
    # Window'u gÃ¶rÃ¼nÃ¼r yap
    window.show()

def update_dataset_options(window):
    """
    SeÃ§ilen kategoriye gÃ¶re mevcut dataset'leri gÃ¼nceller.
    Bu fonksiyon, kullanÄ±cÄ± kategori deÄŸiÅŸtirdiÄŸinde otomatik olarak Ã§alÄ±ÅŸÄ±r.
    """
    category = window.combo_categories.currentText()
    window.combo_datasets.clear()  # Ã–nceki seÃ§enekleri temizle
    
    if category == "Performance Testing":
        # Scalability analysis iÃ§in farklÄ± boyutlarda test verileri
        datasets = [
            "1K Records (157 KB) - Baseline Test",
            "5K Records (786 KB) - Small Scale", 
            "10K Records (1.5 MB) - Medium Scale",
            "50K Records (7.8 MB) - Large Scale",
            "100K Records (15.7 MB) - Enterprise Scale"
        ]
        
    elif category == "Full Production Data":
        # GerÃ§ek analiz iÃ§in complete dataset'ler
        datasets = [
            "PM2.5 Data 2018-2020 (Complete Dataset)",
            "Ozone Data 2018-2020 (Complete Dataset)", 
            "California PM2.5 Data (Regional)",
            "LA Station Time Series (Temporal Analysis)"
        ]
        
    elif category == "Geographic Specific":
        # BÃ¶lgesel analiz iÃ§in filtered veriler
        datasets = [
            "California Only - PM2.5 Measurements",
            "LA Metro Area - Station Network",
        ]
        
    else:  # Manual Path Entry
        # Advanced user'lar iÃ§in custom path option
        datasets = ["Custom Path (Enter Below)"]
        # Manual mode'da user'Ä±n path girmesine izin ver
        window.entry_hdfs_path.setReadOnly(False)
        window.entry_hdfs_path.setStyleSheet("""
            QLineEdit {
                background-color: #1a1a1a;
                color: #ffffff;
                border: 2px solid #4CAF50;
                padding: 4px;
                border-radius: 3px;
            }
        """)
        window.entry_hdfs_path.setPlaceholderText("Enter HDFS path manually...")
        return
    
    # SeÃ§enekleri dropdown'a ekle
    window.combo_datasets.addItems(datasets)
    
    # Automatic mode iÃ§in path'i read-only yap
    window.entry_hdfs_path.setReadOnly(True)
    window.entry_hdfs_path.setStyleSheet("""
        QLineEdit {
            background-color: #2b2b2b;
            color: #ffffff;
            border: 1px solid #555555;
            padding: 4px;
            border-radius: 3px;
        }
        QLineEdit:disabled {
            background-color: #3a3a3a;
            color: #cccccc;
        }
    """)
    
    # Ä°lk dataset seÃ§imini trigger et
    update_hdfs_path_from_selection(window)

def update_hdfs_path_from_selection(window):
    """
    Kategori ve dataset seÃ§imine gÃ¶re HDFS path'ini otomatik olarak gÃ¼nceller.
    Bu mapping, HDFS'teki gerÃ§ek dosya yapÄ±sÄ±nÄ± reflect eder.
    """
    category = window.combo_categories.currentText()
    dataset = window.combo_datasets.currentText()
    
    # Performance testing dataset'leri iÃ§in path mapping
    if category == "Performance Testing":
        if "1K" in dataset:
            path = "/user/hadoop/epa_air_quality/test_data/sample_1000_pm25_performance_test_data.csv"
        elif "5K" in dataset:
            path = "/user/hadoop/epa_air_quality/test_data/sample_5000_pm25_performance_test_data.csv"
        elif "10K" in dataset:
            path = "/user/hadoop/epa_air_quality/test_data/sample_10000_pm25_performance_test_data.csv"
        elif "50K" in dataset:
            path = "/user/hadoop/epa_air_quality/test_data/sample_50000_pm25_performance_test_data.csv"
        elif "100K" in dataset:
            path = "/user/hadoop/epa_air_quality/test_data/sample_100000_pm25_performance_test_data.csv"
        else:
            path = "/user/hadoop/epa_air_quality/test_data/"  # Fallback
    
    # Production dataset'leri iÃ§in path mapping
    elif category == "Full Production Data":
        if "PM2.5 Data 2018-2020" in dataset:
            path = "/user/hadoop/epa_air_quality/raw/optimized_pm25_data_2018_2020.csv"
        elif "Ozone" in dataset:
            path = "/user/hadoop/epa_air_quality/raw/optimized_ozone_data_2018_2020.csv"
        elif "California" in dataset:
            path = "/user/hadoop/epa_air_quality/raw/optimized_california_pm25_data.csv"
        elif "LA Station" in dataset:
            path = "/user/hadoop/epa_air_quality/raw/optimized_la_station_timeseries.csv"
        else:
            path = "/user/hadoop/epa_air_quality/raw/"  # Fallback
    
    # Geographic specific dataset'leri iÃ§in path mapping
    elif category == "Geographic Specific":
        if "California" in dataset:
            path = "/user/hadoop/epa_air_quality/raw/optimized_california_pm25_data.csv"
        elif "LA Metro" in dataset:
            path = "/user/hadoop/epa_air_quality/raw/optimized_la_station_timeseries.csv"
        else:
            path = "/user/hadoop/epa_air_quality/raw/"  # Fallback
    
    else:  # Manual mode
        # Manual mode'da user'Ä±n girmesini bekle
        return
    
    # Computed path'i UI'da gÃ¶ster
    window.entry_hdfs_path.setText(path)

def log_message(window, message):
    window.text_status_log.append(message)
    QApplication.processEvents()

def show_results(window, result_text):
    window.text_results.setText(result_text)
    QApplication.processEvents()

import time

def handle_run_analysis(window):
    # SeÃ§ilen kategoriyi al
    selected_category = window.combo_categories.currentText()
    
    # Performance timing sadece Performance Testing iÃ§in baÅŸlat
    if selected_category == "Performance Testing":
        analysis_start_time = time.time()
        show_performance_metrics = True
        log_message(window, "ğŸ”¬ Performance Testing modu - Timing Ã¶lÃ§Ã¼mÃ¼ aktif")
    else:
        show_performance_metrics = False
        
    log_message(window, "Analiz baÅŸlatÄ±lÄ±yor...")
    
    # SeÃ§ilen fonksiyonu ve HDFS yolunu al
    selected_function = window.combo_functions.currentText()
    hdfs_input_path = window.entry_hdfs_path.text()
    
    # HDFS yolu kontrolÃ¼
    if not hdfs_input_path:
        QMessageBox.warning(window, "GiriÅŸ HatasÄ±", "LÃ¼tfen HDFS giriÅŸ yolunu belirtin.")
        log_message(window, "HATA: HDFS giriÅŸ yolu boÅŸ.")
        return
        
    log_message(window, f"SeÃ§ilen Fonksiyon: {selected_function}")
    log_message(window, f"GiriÅŸ Yolu: {hdfs_input_path}")
    
    # Analiz dÃ¼ÄŸmesini devre dÄ±ÅŸÄ± bÄ±rak
    window.btn_run.setEnabled(False)
    window.text_results.clear()
    QApplication.processEvents()
    
    # MapReduce script hazÄ±rlama sÃ¼resi Ã¶lÃ§Ã¼mÃ¼ (Performance Testing iÃ§in)
    if show_performance_metrics:
        mr_prep_start = time.time()
    
    # SeÃ§ilen fonksiyona gÃ¶re MR scriptlerini EMR'a indir/gÃ¼ncelle
    mr_script_source_s3_path = ""
    emr_mr_script_target_dir = ""
    local_mapper_path_on_emr = ""
    local_reducer_path_on_emr = ""
    hdfs_output_path = ""
    job_name = ""

    if selected_function == "Skewness":
        job_name = "GUI_Skewness_Analysis"
        mr_script_source_s3_path = f"{S3_CODE_BUCKET}/skewness/"
        emr_mr_script_target_dir = "/home/hadoop/mr_scripts_for_gui/skewness"  # Tam yol
        local_mapper_path_on_emr = "skewness_stats_mapper.py"
        local_reducer_path_on_emr = "skewness_stats_reducer.py"
        hdfs_output_path = f"/user/hadoop/epa_air_quality/results/gui_skewness_{selected_function.lower().replace(' ','_')}"
    
    elif selected_function == "Min-Max Normalization":
        # KullanÄ±cÄ±ya hangi aÅŸamayÄ± yapmak istediÄŸini soralÄ±m
        items = ["1. Min-Max DeÄŸerlerini Bul", "2. Normalizasyon Yap"]
        item, ok = QInputDialog.getItem(window, "AÅŸama SeÃ§imi", 
                                    "Min-Max Normalizasyon hangi aÅŸamasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmak istiyorsunuz?", 
                                    items, 0, False)
        
        if ok and item:
            mr_script_source_s3_path = f"{S3_CODE_BUCKET}/min_max/"
            emr_mr_script_target_dir = "/home/hadoop/mr_scripts_for_gui/min_max"
            
            if "1." in item:  # Ä°lk aÅŸama: Min-Max bulma
                job_name = "GUI_MinMax_Find_Values"
                local_mapper_path_on_emr = "min_max_finder_mapper.py"
                local_reducer_path_on_emr = "min_max_finder_reducer.py"
                hdfs_output_path = "/user/hadoop/epa_air_quality/results/gui_minmax_values"
            else:  # Ä°kinci aÅŸama: Normalizasyon
                job_name = "GUI_MinMax_Normalize"
                local_mapper_path_on_emr = "min_max_normalizer_mapper.py"
                local_reducer_path_on_emr = ""  # Bu map-only job
                hdfs_output_path = "/user/hadoop/epa_air_quality/results/gui_normalized_data"
    
    elif selected_function == "Median":
        job_name = "GUI_Median_Analysis"
        # S3'te median klasÃ¶rÃ¼ var mÄ± kontrol etmek gerekebilir
        mr_script_source_s3_path = f"{S3_CODE_BUCKET}/median/"
        emr_mr_script_target_dir = "/home/hadoop/mr_scripts_for_gui/median"
        local_mapper_path_on_emr = "median_histogram_mapper.py"
        local_reducer_path_on_emr = "median_histogram_reducer.py"
        hdfs_output_path = f"/user/hadoop/epa_air_quality/results/gui_median"
    
    elif selected_function == "Standard Deviation":
        job_name = "GUI_StdDev_Analysis"
        mr_script_source_s3_path = f"{S3_CODE_BUCKET}/stddev/"
        emr_mr_script_target_dir = "/home/hadoop/mr_scripts_for_gui/stddev"
        local_mapper_path_on_emr = "stddev_welford_mapper.py"
        local_reducer_path_on_emr = "stddev_welford_reducer.py"
        hdfs_output_path = f"/user/hadoop/epa_air_quality/results/gui_stddev"
    
    elif selected_function == "90th Percentile":
        job_name = "GUI_90th_Percentile_Analysis"
        mr_script_source_s3_path = f"{S3_CODE_BUCKET}/percentile/"
        emr_mr_script_target_dir = "/home/hadoop/mr_scripts_for_gui/percentile"
        local_mapper_path_on_emr = "percentile_90_mapper.py"
        local_reducer_path_on_emr = "percentile_90_reducer.py"
        hdfs_output_path = f"/user/hadoop/epa_air_quality/results/gui_percentile"
    
    else:
        QMessageBox.warning(window, "SeÃ§im HatasÄ±", f"'{selected_function}' iÃ§in MapReduce iÅŸlevi henÃ¼z tanÄ±mlanmadÄ±.")
        log_message(window, f"HATA: '{selected_function}' iÃ§in MR iÅŸlevi yok.")
        window.btn_run.setEnabled(True)
        return

    if mr_script_source_s3_path:
        cmd_list_s3_files = f"aws s3 ls {mr_script_source_s3_path}"
        log_message(window, f"S3'teki dosyalar kontrol ediliyor: {mr_script_source_s3_path}")
        stdout_list, stderr_list = execute_remote_ssh_command(cmd_list_s3_files, window)
        
        if stdout_list:
            log_message(window, f"S3'te bulunan dosyalar:\n{stdout_list}")
        else:
            log_message(window, f"UYARI: S3 yolunda dosya bulunamadÄ± veya eriÅŸilemedi: {stderr_list}")
        
        # MR scriptlerini hazÄ±rla - geliÅŸtirilmiÅŸ hata yakalama ile
        cmd_prepare_mr_scripts = f"""
            # Hedef dizini oluÅŸtur
            mkdir -p {emr_mr_script_target_dir} && \\
            echo "Dizin oluÅŸturuldu: {emr_mr_script_target_dir}" && \\
            
            # S3'ten dosyalarÄ± kopyala
            aws s3 cp {mr_script_source_s3_path} {emr_mr_script_target_dir}/ --recursive && \\
            echo "S3'ten dosyalar kopyalandÄ±" && \\
            
            # Kopyalanan dosyalarÄ± listele
            ls -la {emr_mr_script_target_dir}/ && \\
            
            # Python dosyalarÄ±na Ã§alÄ±ÅŸtÄ±rma izni ver
            if [ -n "$(ls -A {emr_mr_script_target_dir}/*.py 2>/dev/null)" ]; then
                chmod +x {emr_mr_script_target_dir}/*.py && \\
                echo "Python dosyalarÄ±na Ã§alÄ±ÅŸtÄ±rma izni verildi"
            else
                echo "UYARI: Python dosyalarÄ± bulunamadÄ±"
            fi && \\
            
            echo '{selected_function} iÃ§in MR scriptleri EMR master nodeunda hazÄ±rlandÄ±.'
        """
        
        log_message(window, f"{selected_function} iÃ§in MR scriptleri EMR master node'una hazÄ±rlanÄ±yor...")
        stdout, stderr = execute_remote_ssh_command(cmd_prepare_mr_scripts, window)
        if stdout is None:
            log_message(window, f"HATA: MR scriptleri EMR'a hazÄ±rlanamadÄ±. {stderr}")
            window.btn_run.setEnabled(True)
            return
    
    # MR script hazÄ±rlÄ±k sÃ¼resini kaydet (Performance Testing iÃ§in)
    if show_performance_metrics:
        mr_prep_time = time.time() - mr_prep_start
        log_message(window, f"â±ï¸ MR script hazÄ±rlÄ±k sÃ¼resi: {mr_prep_time:.2f} saniye")

    # MapReduce job sÃ¼resi Ã¶lÃ§Ã¼mÃ¼ baÅŸlat (Performance Testing iÃ§in)
    if show_performance_metrics:
        mapreduce_start = time.time()

    # HDFS output dizinini silme komutu
    cmd_delete_hdfs_output_on_emr = f"hdfs dfs -rm -r {hdfs_output_path} 2>/dev/null || true"
    log_message(window, f"Eski HDFS Ã§Ä±ktÄ± dizini '{hdfs_output_path}' siliniyor (eÄŸer varsa)...")
    stdout_del, stderr_del = execute_remote_ssh_command(cmd_delete_hdfs_output_on_emr, window)

    # Hadoop streaming komutunu oluÅŸtur
    # Ã–nce STREAMING_JAR'Ä±n yerini bulalÄ±m
    cmd_find_streaming_jar = "find /usr/lib/hadoop-mapreduce/ -name 'hadoop-streaming*.jar' | head -1"
    log_message(window, "Hadoop streaming JAR dosyasÄ± aranÄ±yor...")
    stdout_jar, stderr_jar = execute_remote_ssh_command(cmd_find_streaming_jar, window)
    
    if stdout_jar and stdout_jar.strip():
        streaming_jar_path = stdout_jar.strip()
        log_message(window, f"Streaming JAR bulundu: {streaming_jar_path}")
    else:
        log_message(window, "HATA: Hadoop streaming JAR dosyasÄ± bulunamadÄ±!")
        window.btn_run.setEnabled(True)
        return
    
    # Hadoop komutunu oluÅŸtur
    hadoop_command_parts = [
        'hadoop', 'jar', streaming_jar_path,
        '-D', f'mapreduce.job.name={job_name}',
    ]
    
    # Reducer sayÄ±sÄ±nÄ± ayarla
    if selected_function in ["Skewness", "Min-Max Normalization", "Median", "Standard Deviation", "90th Percentile"]:
        hadoop_command_parts.extend(['-D', 'mapreduce.job.reduces=1'])
    
    # Scriptlerin EMR Ã¼zerindeki tam yollarÄ±
    abs_mapper_on_emr = f"{emr_mr_script_target_dir}/{local_mapper_path_on_emr}"
    
    # -files argÃ¼manÄ± iÃ§in yollar
    files_for_hadoop_cmd = [abs_mapper_on_emr]
    if local_reducer_path_on_emr and local_reducer_path_on_emr != "None":
        abs_reducer_on_emr = f"{emr_mr_script_target_dir}/{local_reducer_path_on_emr}"
        files_for_hadoop_cmd.append(abs_reducer_on_emr)

    if local_reducer_path_on_emr and local_reducer_path_on_emr != "":
        hadoop_command_parts.extend(['-reducer', f'./{local_reducer_path_on_emr}'])
    else:
        # Map-only job iÃ§in reducer sayÄ±sÄ±nÄ± 0 yap
        hadoop_command_parts.extend(['-D', 'mapreduce.job.reduces=0'])

    for file_path in files_for_hadoop_cmd:
        hadoop_command_parts.extend(['-file', file_path])

    if selected_function == "Min-Max Normalization" and "2." in item:
        minmax_result_path = "/user/hadoop/epa_air_quality/results/gui_minmax_values/part-00000"
        cmd_read_minmax = f"hdfs dfs -cat {minmax_result_path}"
        log_message(window, "Min-Max deÄŸerleri Ã¶nceki job'dan okunuyor...")
        minmax_output, minmax_stderr = execute_remote_ssh_command(cmd_read_minmax, window)

        if minmax_output is None:  
            log_message(window, f"HATA: Min-Max deÄŸerleri okunamadÄ±. Ã–nce 1. aÅŸamayÄ± Ã§alÄ±ÅŸtÄ±rÄ±n. {minmax_stderr}")
            window.btn_run.setEnabled(True)
            return
        
        try:
            lines = minmax_output.strip().split('\n')
            global_min = None
            global_max = None
            for line in lines:
                if line.startswith('global_min'):
                    global_min = float(line.split('\t')[1])
                elif line.startswith('global_max'):
                    global_max = float(line.split('\t')[1])
            
            if global_min is None or global_max is None:
                log_message(window, "HATA: Min-Max deÄŸerleri parse edilemedi.")
                window.btn_run.setEnabled(True)
                return
            
            log_message(window, f"Dinamik Min-Max deÄŸerleri: min={global_min}, max={global_max}")
        
        except Exception as parse_error:
            log_message(window, f"HATA: Min-Max deÄŸerleri parse edilirken hata: {parse_error}")
            window.btn_run.setEnabled(True)
            return
        mapper_command_with_params = f'./{local_mapper_path_on_emr} {global_min} {global_max}'
        hadoop_command_parts.extend(['-mapper', mapper_command_with_params])
    else:
        hadoop_command_parts.extend(['-mapper', f'./{local_mapper_path_on_emr}'])

    if local_reducer_path_on_emr and local_reducer_path_on_emr != "None":
        hadoop_command_parts.extend(['-reducer', f'./{local_reducer_path_on_emr}'])
    
    hadoop_command_parts.extend(['-input', hdfs_input_path])
    hadoop_command_parts.extend(['-output', hdfs_output_path])

    # Hadoop komutunu Ã§alÄ±ÅŸtÄ±r
    final_hadoop_command_on_emr = ' '.join(shlex.quote(c) for c in hadoop_command_parts)
    log_message(window, "Hadoop streaming iÅŸi EMR Ã¼zerinde baÅŸlatÄ±lÄ±yor...")
    stdout_mr, stderr_mr = execute_remote_ssh_command(final_hadoop_command_on_emr, window)

    # MapReduce sÃ¼resini kaydet (Performance Testing iÃ§in)
    if show_performance_metrics:
        mapreduce_time = time.time() - mapreduce_start
        log_message(window, f"â±ï¸ MapReduce iÅŸlem sÃ¼resi: {mapreduce_time:.2f} saniye")

    # Ä°ÅŸin baÅŸarÄ±lÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
    application_id = None
    job_successful = False
    
    if stderr_mr is not None and "completed successfully" in stderr_mr.lower():
        job_successful = True
        # Application ID'yi bulmaya Ã§alÄ±ÅŸ
        for line in stderr_mr.splitlines():
            if "Submitted application" in line:
                try:
                    app_id_part = line.split("Submitted application")[1].strip()
                    if app_id_part:
                        application_id = app_id_part.split()[0]
                        log_message(window, f"Yakalanan YARN App ID: {application_id}")
                        break
                except:
                    pass
        log_message(window, f"MapReduce iÅŸi '{job_name}' EMR Ã¼zerinde baÅŸarÄ±yla tamamlandÄ±.")
    elif stdout_mr is None:
        log_message(window, f"HATA: MapReduce iÅŸi '{job_name}' EMR Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±lamadÄ±. {stderr_mr}")
    else:
        log_message(window, f"HATA: MapReduce iÅŸi '{job_name}' EMR Ã¼zerinde hata ile sonlandÄ±.")

    # SonuÃ§larÄ± HDFS'ten oku
    if job_successful:
        log_message(window, "SonuÃ§lar HDFS'ten okunuyor...")
        result_file_hdfs_path = f"{hdfs_output_path}/part-00000"
        
        cmd_read_results_on_emr = f"hdfs dfs -cat {result_file_hdfs_path}"
        results_content, stderr_read = execute_remote_ssh_command(cmd_read_results_on_emr, window)
        
        if results_content:
            log_message(window, "SonuÃ§lar baÅŸarÄ±yla okundu.")
            
            # Performance Testing iÃ§in detaylÄ± sonuÃ§ gÃ¶sterimi
            if show_performance_metrics and selected_category == "Performance Testing":
                analysis_end_time = time.time()
                total_duration = analysis_end_time - analysis_start_time
                
                # Dataset'ten kayÄ±t sayÄ±sÄ±nÄ± Ã§Ä±karmaya Ã§alÄ±ÅŸ
                dataset_text = window.combo_datasets.currentText()
                processed_records = 0
                if "1K" in dataset_text:
                    processed_records = 1000
                elif "5K" in dataset_text:
                    processed_records = 5000
                elif "10K" in dataset_text:
                    processed_records = 10000
                elif "50K" in dataset_text:
                    processed_records = 50000
                elif "100K" in dataset_text:
                    processed_records = 100000
                
                # Performance Ã¶zeti ekle
                enhanced_results = results_content + "\n\n" + "="*60 + "\n"
                enhanced_results += f"ğŸ”¬ PERFORMANCE ANALYSIS RESULTS\n"
                enhanced_results += f"ğŸ“Š Total Execution Time: {total_duration:.2f} seconds\n"
                enhanced_results += f"ğŸ“ˆ Dataset: {dataset_text}\n"
                if processed_records > 0:
                    enhanced_results += f"âš¡ Processing Rate: {processed_records/total_duration:.0f} records/sec\n"
                enhanced_results += f"\nDetailed Timing Breakdown:\n"
                enhanced_results += f"   â€¢ MR Script Preparation: {mr_prep_time:.2f} seconds\n"
                enhanced_results += f"   â€¢ MapReduce Execution: {mapreduce_time:.2f} seconds\n"
                enhanced_results += "="*60
                
                log_message(window, f"â±ï¸ Performance Test tamamlandÄ±: {total_duration:.2f} saniye")
                show_results(window, enhanced_results)
            else:
                # DiÄŸer kategoriler iÃ§in sade sonuÃ§ gÃ¶sterimi  
                log_message(window, "âœ… Analiz baÅŸarÄ±yla tamamlandÄ±")
                show_results(window, results_content)
        else:
            log_message(window, f"HATA: SonuÃ§lar HDFS'ten okunamadÄ±. {stderr_read}")
            show_results(window, f"HATA: SonuÃ§lar HDFS'ten okunamadÄ±.\n{stderr_read}")
    else:
        show_results(window, "MapReduce iÅŸi baÅŸarÄ±sÄ±z olduÄŸu iÃ§in sonuÃ§lar okunamadÄ±.")

    window.btn_run.setEnabled(True)
    log_message(window, "Analiz iÅŸlemi tamamlandÄ±.")

def main():
    global app
    app = QApplication(sys.argv)
    main_window = QWidget()
    init_ui(main_window)
    sys.exit(app.exec_())

if __name__ == '__main__':
    main()